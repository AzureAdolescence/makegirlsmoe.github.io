---
layout: default
title:  "Create Anime Characters with A.I. !"
date:   2017-08-14 06:03:00
categories: main
---

# Create Anime Characters with A.I. !

We all love anime characters and are tempted to create our own,
but most of us cannot do that because we are not professional artists.
What if anime characters could be generated automatically at a professional level of quality?  Imagine that you could just specify attributes (such as blonde/twin tailed/smiling), and have an anime character with your customizations generated without any further intervention!

In the community there are already several pioneering works in anime image generation, such as [ChainerDCGAN](https://github.com/pfnet-research/chainer-gan-lib), [Chainerを使ってコンピュータにイラストを描かせる](http://qiita.com/rezoolab/items/5cc96b6d31153e0c86bc), and projects like [IllustrationGAN](https://github.com/tdrussell/IllustrationGAN) and [AnimeGAN](https://github.com/jayleicn/animeGAN) which have their code available online.
However, the results generated by these models are often [blurred](https://github.com/jayleicn/animeGAN/blob/master/images/fake_sample.png) and [distorted](https://qiita-image-store.s3.amazonaws.com/0/61296/7838e32d-1ca9-be96-ddd9-2e400be99ea1.jpeg),
and generating industry-standard facial images for anime characters remains an unsolved problem.

As a step towards tackling this challenge, we propose a model that produces high quality anime faces with a promising rate of success.

#### Dataset: a Good Quality Model Begins with a Clean Dataset.

To teach computers to do things requires high quality data, and our case is not an exception.
The quality of images on large scale image boards like [Danbooru](https://danbooru.donmai.us) and [Safebooru](https://safebooru.org) varies wildly, and we think this is at least part of the reason for the quality issues in previous works.
So, instead, we use "standing pictures" ([立ち絵](http://dic.nicovideo.jp/a/%E7%AB%8B%E3%81%A1%E7%B5%B5)) from games sold on [Getchu](http://www.getchu.com), a website for learning about and purchasing Japanese games.
Standing pictures are diverse since they are rendered in different styles for different genres of game, yet reasonably consistent since they are all part of the domain of game character images.

We also need categorical metadata (a.k.a tags/attributes) for the images, like hair color, and whether faces are smiling or not.
Getchu does not provide such metadata, so we use [Illustration2Vec](http://illustration2vec.net/), a [CNN](https://cs231n.github.io/convolutional-networks/)-based tool for predicting anime image tags.

#### Model: The Essential Part

A good generative model is also a must-have for our goal.
The generator should understand and follow the user's specified attributes, which is called our _prior_,
and it should also have the freedom to generate different, detailed visual features, which is modeled using _noise_.
We use a popular framework called GANs ([Generative Adversarial Networks](https://papers.nips.cc/paper/5423-generative-adversarial-nets)) to accomplish this.

GANs use a generator network $$G$$ to generate images from the _prior_ and _noise_ inputs,
and also another network $$D$$ which tries to distinguish $$G$$'s generated images from real images.
We train them both, and in the end $$G$$ should be able to generate images so realistic that $$D$$ cannot differentiate them from real images with that _prior_.
However, training GANs properly is infamously hard and time-consuming.
Luckily a recent advance, named [DRAGAN](https://arxiv.org/abs/1705.07215),
can give plausible results compared to other GANs with comparatively little computational power required. We successfully train a DRAGAN whose generator is [SRResNet](https://arxiv.org/abs/1609.04802)-like.

We also need our generator to know the label information, so that the user's specifications can be incorporated. Inspired by [ACGAN](https://arxiv.org/abs/1610.09585),
we feed the labels to the generator $$G$$, along with noise, and add a multi-label classifier on the top of the discriminator $$D$$ which is asked to predict the assigned tags for the images.

With this data and model, we train straightforwardly on [GPU-powered](http://www.nvidia.com/object/machine-learning.html) machines.

#### Samples: A Picture is Worth a Thousand Words

To sample the quality of our model, see generated images like the following: it handles different attributes and visual features well.

<center><img src="{{ site.url }}/assets/news-img/samples.jpg" align="middle" width="500"></center>

One interesting setting is fixing the random _noise_ part and sampling random _priors_. The model is now required to generate images with similar major visual features and different attribute combinations, and it does this successfully:

<center><img src="{{ site.url }}/assets/news-img/fixed_noise.jpg" align="middle" width="500"></center>

Also, by fixing _priors_ and sampling randomly for the _noise_, the model can generate images which have the same attributes with different visual features:

<center><img src="{{ site.url }}/assets/news-img/fix_attributes_a.png" align="middle" width="500"></center>

#### Web Interface: Bringing the Neural Generator to your Browser

In order to make our model more accessible, we built a [website interface](http://make.girls.moe) with [React.js](https://facebook.github.io/react/) for open access.
We do the generation entirely client-side by using [WebDNN](https://mil-tokyo.github.io/webdnn/) and converting the trained [Chainer](https://chainer.org/) model to a [WebAssembly](http://webassembly.org/)-based Javascript model.
For a better user experience, we want to keep the size of the generator model small (since users need to download the model before generating anything),
and our choice of an [SRResNet](https://arxiv.org/abs/1609.04802) generator makes the model $$4$$ times smaller than the popular [DCGAN](https://arxiv.org/abs/1511.06434) generator without compromising on the quality of results.
Speed-wise, even though all computations are done on the client side, it still only takes about $$6\sim 7$$ seconds on average to generate an image.

#### For more technical details, check out our paper on [arXiv](https://arxiv.org/abs/1708.05509) which is initially available as the [techinical report]({{ site.url }}/assets/pdf/technical_report.pdf).
